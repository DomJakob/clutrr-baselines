---
general:
  seed: 42
  base_path: ''
  device: cuda:0
  id: GRAPHSUM1
  exp_name: default
  description: Sample code. Does not train any model
  commit_id: ''
  mode: sample
  only_infer: False # set this to true if we want to run only the inference on test data
  comet:
    api_key: # put the comet api key here
    project_name: # comet project name
    workspace: # comet workspace
dataset:
  base_path: data
  filename: output.csv
  save_path: data_saved.pkl
  load_save_path: true
  name: family
  is_preprocessed: true
  train_test_split: 0.8
  train_val_split: 0.8
  max_vocab: -1
  tokenization: word
  common_dict: true
  sentence_mode: false
  single_abs_line: true  # makes every story-abstract pair with abstracts as single lines
  only_relation: false  # set this value to true when testing only relations as single words. Only works in toy data.
model:
  name: baseline1
  batch_size: 100
  num_epochs: 20
  persist_per_epoch: -1
  early_stopping_patience: 1
  save_dir: ''
  should_load_model: false
  dropout_probability: 0
  tf_ratio: 1
  loss_criteria: CE
  loss_type: classify   # set this to classify when performing a classification task, else `seq2seq`
  query_entities: 2
  only_relation: false  # set this to true when dataset.only_relation is true
  early_stopping:
    patience: 3
    metric_to_track: loss
  embedding:
    dim: 100
    should_use_pretrained_embedding: false
    should_finetune_embedding: true
    pretrained_embedding_path: w2v/w2v_sst.txt
  optimiser:
    name: adam
    learning_rate: 0.001
    scheduler_type: exp
    scheduler_gamma: 1
    scheduler_patience: 10
    l2_penalty: 0
  encoder:
    name: codes.baselines.seq2seq.SimpleEncoder
    hidden_dim: 100
    nlayers: 2
    bidirectional: true
    dropout: 0
    tmp_name: codes.models.gnn.encoder.GraphEncoder
    invalidate_embeddings: True
  decoder:
    name: codes.baselines.seq2seq.SimpleDecoder
    hidden_dim: 200
    nlayers: 2
    bidirectional: false
    dropout: 0
    query_ents: 2
    tmp_name: codes.models.gnn.decoder.GraphLSTMDecoder
    invalidate_embeddings: False
  beam:
    beam_size: 3
    alpha: 0
    beta: 0
    coverage_penalty: 0
    length_penalty: 0
    max_length: 25
    n_best: 1
  graph:
    node_dim: 100
    message_dim: 100
    edge_dim: 100
    pos_rep: random # if random, then use a frozen embedding layer to denote the node position. if one-hot, then use just a one-hot embedding
    pos_dim: 50 # if above is random, then use this flag
    feature_dim: 10 # learns graph features
    num_reads: 1
    num_message_rounds: 2
    message_function:
      num_layers: 2
    readout_function:
      num_layers: 2
    update_function:
      num_layers: 2
    edge_embedding: lstm # if edge_embedding is lstm, then use an lstm to extract the relation, else average word embedding
log:
  file_path: ''
  mongo_host: 127.0.0.1
  mongo_port: '8092'
  mongo_db: graphsum
  logs_per_epoch: 50
  use_mongo: false
plot:
  base_path: ''
